{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66440231-6e48-4753-8834-eccd7436d91f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46b94bf4-6360-40e7-96fb-55595dc0c3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Process Data\n",
    "\n",
    "This notebook takes all the raw files from the bronze volume and processes them into a single table. It could be optimized to use less ram, but does the trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68573d94-ca32-4cd5-ac34-eddec6f79718",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from well_agent.utils import get_config_path, DotConfig\n",
    "config_path = get_config_path()\n",
    "config_path = 'config.yaml'\n",
    "config = DotConfig(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.connect import DatabricksSession as SparkSession\n",
    "\n",
    "if \"spark\" not in locals():\n",
    "    spark = SparkSession.builder.serverless(True).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07807dc0-5200-4b75-98ab-349ad87abef5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Bronze to Silver Cleaning\n",
    "There are a couple problems with how the files were saved for use with spark. We need to loop through every parquet and fix the timestamp before reading with spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8b6b31-fab2-4c9a-91d1-73c37da975d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "cols_to_keep = ['P-PDG', 'P-MON-CKP', 'P-MON-CKGL', 'P-JUS-CKGL', 'P-JUS-CKP', 'P-TPT', 'T-TPT', 'QGL', 'T-JUS-CKP', 'state', 'timestamp']\n",
    "\n",
    "for path in Path(config.download.output_dir).rglob('*.parquet'):\n",
    "  df_in = pd.read_parquet(path)\n",
    "  df_out = df_in.reset_index()[cols_to_keep].dropna(subset=['state'])\n",
    "  out_path = str(path).replace('bronze','silv')\n",
    "  \n",
    "  # check if the modified file exists already\n",
    "  if Path(out_path).exists():\n",
    "    next\n",
    "  else:\n",
    "    print(path)\n",
    "  \n",
    "  # save the fixed file out\n",
    "  Path(out_path).parent.mkdir(exist_ok=True)\n",
    "  df_out.to_parquet(\n",
    "    Path(out_path),\n",
    "    coerce_timestamps=\"ms\", \n",
    "    allow_truncated_timestamps=True,\n",
    "    index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d989d890-0cff-4338-befc-17913d85dae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Generation\n",
    "\n",
    "Now that we have proper files, we can use spark to make an enormous table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0249ea2a-8a29-4eb9-b344-7e55c90257a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, DoubleType, IntegerType, TimestampType, LongType, StringType\n",
    ")\n",
    "\n",
    "SCHEMA_3W = StructType([\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"P-PDG\", DoubleType(), True),\n",
    "    StructField(\"P-MON-CKP\", DoubleType(), True),\n",
    "    StructField(\"P-JUS-CKP\", DoubleType(), True),\n",
    "    StructField(\"P-TPT\", DoubleType(), True),\n",
    "    StructField(\"T-TPT\", DoubleType(), True),\n",
    "    StructField(\"QGL\", DoubleType(), True),\n",
    "    StructField(\"T-JUS-CKP\", DoubleType(), True),\n",
    "    StructField(\"state\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b737fec8-e87d-4dc0-a705-bd3312923d0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path_pattern = f\"{config.process.silver_dir}**/*.parquet\"\n",
    "df = spark.read.schema(SCHEMA_3W).parquet(path_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e8f8588-a093-4e2b-ac53-663652a41892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, sum as spark_sum, isnan\n",
    "\n",
    "na_counts = df.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns\n",
    "])\n",
    "display(na_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fde15f0a-5cf7-445d-90ac-3982b69b15fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from well_agent.process import clean_data_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d13f7755-5bab-4a92-8c0f-1654d55ae94a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.withColumn(\"source_file\", df[\"_metadata.file_path\"])\n",
    "df = clean_data_spark(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d350dddb-bda8-4f5a-9c78-79f5ee1bb28e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## forward fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "613b13f9-d6eb-4f7d-b382-9176394d6b7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "well_col = \"well_number\"  # adjust if your well column is named differently\n",
    "timestamp_col = \"timestamp\"\n",
    "\n",
    "window_spec = Window.partitionBy(well_col).orderBy(timestamp_col).rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "ffill_exprs = [\n",
    "    F.last(F.col(c), ignorenulls=True).over(window_spec).alias(c)\n",
    "    if c not in [well_col, timestamp_col] else F.col(c)\n",
    "    for c in df.columns\n",
    "]\n",
    "\n",
    "df = df.select(ffill_exprs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dc81c77-ee0c-46c1-8f00-3771d8d7b023",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    df\n",
    "    .write.mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{config.catalog}.{config.schema}.{config.process.table}\")\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "2_process",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
