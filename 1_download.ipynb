{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29074c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serverless 3W Dataset Downloader Job\n",
    "# This job downloads parquet files from the Petrobras 3W dataset to a Databricks volume\n",
    "\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Get config path from job parameters\n",
    "dbutils.widgets.text(\"config_path\", \"config.yaml\")\n",
    "config_path = dbutils.widgets.get(\"config_path\")\n",
    "\n",
    "print(f\"Using config file: {config_path}\")\n",
    "\n",
    "# Simple config loader since we can't use the src modules\n",
    "import yaml\n",
    "with open(config_path, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n",
    "\n",
    "# Override output directory to use volume\n",
    "output_dir = \"/Volumes/shm/3w/bronze/\"\n",
    "max_files = config['download'].get('max_files', 10)\n",
    "max_dirs = config['download'].get('max_dirs', 1)\n",
    "base_url = config['download']['base_url']\n",
    "delay_seconds = config['download'].get('delay_seconds', 0.1)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Max files: {max_files}\")\n",
    "print(f\"Max dirs: {max_dirs}\")\n",
    "print(f\"Base URL: {base_url}\")\n",
    "\n",
    "# Download function\n",
    "session = requests.Session()\n",
    "\n",
    "def get_directory_contents(path=\"dataset\"):\n",
    "    \"\"\"Get contents of a directory from GitHub API.\"\"\"\n",
    "    url = f\"{base_url}/{path}\"\n",
    "    response = session.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error fetching {url}: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "def download_file(download_url, local_path):\n",
    "    \"\"\"Download a single file.\"\"\"\n",
    "    try:\n",
    "        # Ensure parent directory exists\n",
    "        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n",
    "        \n",
    "        # Check if file already exists\n",
    "        if os.path.exists(local_path):\n",
    "            return True\n",
    "        \n",
    "        print(f\"Downloading: {os.path.basename(local_path)}\")\n",
    "        response = session.get(download_url, stream=True)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            with open(local_path, \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(f\"✓ Downloaded: {local_path}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"✗ Failed to download {download_url}: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error downloading {download_url}: {e}\")\n",
    "        return False\n",
    "\n",
    "# Get all parquet files\n",
    "print(\"Getting file list from 3W dataset...\")\n",
    "subdirs = get_directory_contents(\"dataset\")\n",
    "\n",
    "if max_dirs and max_dirs > 0:\n",
    "    subdirs = subdirs[:max_dirs]\n",
    "    print(f\"Limited to first {max_dirs} directories\")\n",
    "\n",
    "files = []\n",
    "for subdir in subdirs:\n",
    "    if subdir.get(\"type\") == \"dir\":\n",
    "        subdir_name = subdir[\"name\"]\n",
    "        print(f\"Scanning directory: {subdir_name}\")\n",
    "        \n",
    "        subdir_files = get_directory_contents(f\"dataset/{subdir_name}\")\n",
    "        for file_info in subdir_files:\n",
    "            if file_info.get(\"type\") == \"file\" and file_info.get(\"name\", \"\").endswith(\".parquet\"):\n",
    "                files.append({\n",
    "                    \"name\": file_info[\"name\"],\n",
    "                    \"path\": file_info[\"path\"],\n",
    "                    \"download_url\": file_info[\"download_url\"],\n",
    "                    \"size\": file_info[\"size\"],\n",
    "                    \"subdir\": subdir_name,\n",
    "                })\n",
    "\n",
    "if max_files and max_files > 0:\n",
    "    files = files[:max_files]\n",
    "    print(f\"Limited to first {max_files} files\")\n",
    "\n",
    "print(f\"Will download {len(files)} parquet files\")\n",
    "\n",
    "# Download files\n",
    "successful_downloads = 0\n",
    "total_size = 0\n",
    "\n",
    "for i, file_info in enumerate(files, 1):\n",
    "    local_path = os.path.join(output_dir, file_info[\"subdir\"], file_info[\"name\"])\n",
    "    \n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"[{i}/{len(files)}] Skipping existing file: {file_info['name']}\")\n",
    "        successful_downloads += 1\n",
    "        continue\n",
    "    \n",
    "    print(f\"[{i}/{len(files)}] Downloading {file_info['name']} ({file_info['size']} bytes)\")\n",
    "    \n",
    "    if download_file(file_info[\"download_url\"], local_path):\n",
    "        successful_downloads += 1\n",
    "        total_size += file_info[\"size\"]\n",
    "    \n",
    "    time.sleep(delay_seconds)\n",
    "\n",
    "print(f\"\\nDownload complete!\")\n",
    "print(f\"Successfully downloaded: {successful_downloads}/{len(files)} files\")\n",
    "print(f\"Total data size: {total_size / (1024*1024):.2f} MB\")\n",
    "print(f\"Files saved to: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506f9e9f",
   "metadata": {},
   "source": [
    "# Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937445f8",
   "metadata": {},
   "source": [
    "Databricks job for downloading Petrobras 3W dataset files. This job reads configuration from config.yaml and downloads the 3W dataset according to the specified parameters. Designed to run as a Databricks job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb99fa",
   "metadata": {},
   "source": [
    "## Download Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8165c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the uploaded source directory to Python path\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/Workspace/Users/scott.mckean@databricks.com/.bundle/hydrate/dev/files')\n",
    "\n",
    "# Install dependencies\n",
    "%pip install pyyaml requests pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb5db2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.github.com/repos/petrobras/3W/contents'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.download import DatasetDownloader\n",
    "from src.utils import DotConfig\n",
    "\n",
    "# Get config path from job parameters\n",
    "dbutils.widgets.text(\"config_path\", \"config.yaml\")\n",
    "config_path = dbutils.widgets.get(\"config_path\")\n",
    "\n",
    "print(f\"Using config file: {config_path}\")\n",
    "\n",
    "config = DotConfig(config_path)\n",
    "\n",
    "# Override output directory to use volume for jobs\n",
    "config.download.output_dir = \"/Volumes/shm/3w/bronze/\"\n",
    "\n",
    "print(f\"Base URL: {config.download.base_url}\")\n",
    "print(f\"Output directory: {config.download.output_dir}\")\n",
    "print(f\"Max files: {config.download.max_files}\")\n",
    "print(f\"Max dirs: {config.download.max_dirs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cc605d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded from config\n",
      "Output directory: ./3w_dataset\n",
      "Max files setting: 10 (type: <class 'int'>)\n",
      "Max directories setting: 1 (type: <class 'int'>)\n",
      "Will limit to 10 files\n",
      "Will limit to 1 directories\n",
      "Getting file list from 3W dataset...\n",
      "Found 14 total directories\n",
      "Applying max_dirs limit: 1\n",
      "Limited to first 1 directories\n",
      "Scanning directory: 0\n",
      "Found 594 total parquet files\n",
      "Applying max_files limit: 10\n",
      "Limited to first 10 files\n",
      "Will download 10 parquet files\n",
      "[1/10] Skipping existing file: WELL-00001_20170201010207.parquet\n",
      "[2/10] Skipping existing file: WELL-00001_20170201060114.parquet\n",
      "[3/10] Skipping existing file: WELL-00001_20170201110124.parquet\n",
      "[4/10] Skipping existing file: WELL-00001_20170201160311.parquet\n",
      "[5/10] Skipping existing file: WELL-00001_20170201210228.parquet\n",
      "[6/10] Skipping existing file: WELL-00001_20170202020343.parquet\n",
      "[7/10] Skipping existing file: WELL-00001_20170202070239.parquet\n",
      "[8/10] Skipping existing file: WELL-00001_20170218000146.parquet\n",
      "[9/10] Skipping existing file: WELL-00001_20170218050218.parquet\n",
      "[10/10] Skipping existing file: WELL-00001_20170218100218.parquet\n",
      "\n",
      "Download complete!\n",
      "Successfully processed: 10/10 files\n",
      "Skipped existing files: 10\n",
      "New downloads: 0\n",
      "Total new data size: 0.00 MB\n",
      "Files saved to: /Users/scott.mckean/Repos/hydrate_prediction/3w_dataset\n"
     ]
    }
   ],
   "source": [
    "downloader = DatasetDownloader(config)\n",
    "downloader.download_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
